# Pose Interaction Frontend - Percussion MVP

A real-time gesture-based percussion system using pose detection. Play virtual drums by moving your hands in front of the camera!

## üéØ Features

### Core Functionality
- **Real-time Pose Detection**: Uses TensorFlow.js MoveNet (Lightning/Thunder) for accurate body tracking
- **Gesture-Based Drumming**: Stroke-based detection for natural drumming interaction
  - **Left hand downward stroke** ‚Üí Hi-Hat sound
  - **Right hand downward stroke** ‚Üí Snare drum
  - **Both hands simultaneously** ‚Üí Kick drum
- **Visual Feedback**: 
  - Animated drum pads on screen with hit flash effects
  - Real-time skeleton overlay
  - Hit statistics display
- **Audio System**: WebAudio API with velocity-mapped volume and pitch variation
- **Performance Optimized**: Frame-rate independent velocity calculation with EMA smoothing

### Technical Features
- **GPU Acceleration**: Automatic backend selection (WebGPU ‚Üí WebGL ‚Üí WASM ‚Üí CPU)
- **Adaptive Processing**: Frame-rate independent gesture detection
- **Smart Smoothing**: EMA (Exponential Moving Average) with alpha=0.7 for responsive yet stable tracking
- **Stroke-Based Detection**: Accumulates vertical displacement over time (like real drumming)
- **Privacy-Conscious**: Camera feed with blur filter (configurable)

## üõ†Ô∏è Tech Stack

- **Frontend**: React 18 + TypeScript + Vite
- **ML Framework**: TensorFlow.js + @tensorflow-models/pose-detection
- **Audio**: WebAudio API with dynamic compression
- **Styling**: Modern CSS with Dartmouth Green theme
- **Testing**: Vitest + React Testing Library
- **Code Quality**: ESLint + TypeScript strict mode

## üìã Prerequisites

- **Node.js** >= 18.x
- **npm** or **pnpm**
- **Modern browser** with:
  - WebGL/WebGPU support
  - WebAudio API support
  - Webcam access
- **Good lighting** for optimal pose detection

## üöÄ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/YunmaoLeo/multi-model-input-web-app.git
cd PoseInteractionFrontEnd

# Install dependencies
npm install
```

### 2. Prepare Audio Files

**Option A: Generate Test Samples**
```bash
cd public/assets/drums
pip install numpy scipy
python generate_drums.py
cd ../../..
```

**Option B: Use Professional Samples**
Place the following WAV files in `public/assets/drums/`:
- `kick.wav` - Kick drum (bass drum)
- `snare.wav` - Snare drum
- `hihat.wav` - Hi-hat cymbal

### 3. Run Development Server

```bash
npm run dev
```

Open `http://localhost:5173` in your browser.

## üéÆ How to Use

### Getting Started
1. **Start Camera**: Click "Start Camera" to enable webcam
2. **Grant Permission**: Allow camera access when prompted
3. **Start Detection**: Click "Start" button to begin pose detection
4. **Audio Initialization**: Audio system activates automatically on first start

### Playing the Drums
1. **Position yourself**: Stand 1-2 meters from the camera
2. **Raise your hands**: Position them comfortably at chest/shoulder height
3. **Make downward strokes**: Quick downward hand motions trigger drum sounds
   - **Left hand** ‚Üí Hi-Hat (high-pitched cymbal)
   - **Right hand** ‚Üí Snare (crisp drum)
   - **Both hands together** ‚Üí Kick (deep bass)

### Tips for Best Experience
- ‚úÖ Ensure good, even lighting
- ‚úÖ Make clear, decisive downward motions
- ‚úÖ Keep upper body visible to camera
- ‚úÖ Stroke distance: ~15-20cm minimum vertical movement
- ‚úÖ Stroke timeout: Complete motion within 500ms
- ‚ö†Ô∏è Avoid slow, gradual movements
- ‚ö†Ô∏è Don't stand too close to the camera

## üìÅ Project Structure

```
src/
‚îú‚îÄ‚îÄ components/              # React components
‚îÇ   ‚îú‚îÄ‚îÄ AudioWaveform.tsx   # (Deprecated - kept for compatibility)
‚îÇ   ‚îú‚îÄ‚îÄ CameraFeed.tsx      # Webcam capture with blur effect
‚îÇ   ‚îú‚îÄ‚îÄ ControlPanel.tsx    # Model & threshold controls
‚îÇ   ‚îú‚îÄ‚îÄ PoseOverlay.tsx     # Skeleton + drum pad visualization
‚îÇ   ‚îî‚îÄ‚îÄ StatusBar.tsx       # Performance metrics
‚îÇ
‚îú‚îÄ‚îÄ lib/                     # Core business logic
‚îÇ   ‚îú‚îÄ‚îÄ audio.ts            # WebAudio manager (playback, velocity mapping)
‚îÇ   ‚îú‚îÄ‚îÄ fps.ts              # Performance monitoring
‚îÇ   ‚îú‚îÄ‚îÄ gesture.ts          # Stroke-based gesture detection
‚îÇ   ‚îú‚îÄ‚îÄ pose.ts             # MoveNet wrapper
‚îÇ   ‚îî‚îÄ‚îÄ smooth.ts           # EMA keypoint smoothing
‚îÇ
‚îú‚îÄ‚îÄ pages/
‚îÇ   ‚îî‚îÄ‚îÄ App.tsx             # Main application orchestrator
‚îÇ
‚îú‚îÄ‚îÄ styles/
‚îÇ   ‚îî‚îÄ‚îÄ global.css          # Global styles & theme
‚îÇ
‚îî‚îÄ‚îÄ types/
    ‚îî‚îÄ‚îÄ pose.d.ts           # TypeScript type definitions

public/
‚îî‚îÄ‚îÄ assets/
    ‚îî‚îÄ‚îÄ drums/              # Audio samples
        ‚îú‚îÄ‚îÄ generate_drums.py
        ‚îú‚îÄ‚îÄ kick.wav
        ‚îú‚îÄ‚îÄ snare.wav
        ‚îî‚îÄ‚îÄ hihat.wav
```

## ‚öôÔ∏è Configuration

### Gesture Detection Parameters
Located in `src/lib/gesture.ts`:

```typescript
{
  speedThreshold: 0.00015,        // (Deprecated) velocity threshold
  displacementThreshold: 0.08,    // Stroke distance (8% of screen height)
  strokeTimeoutMs: 500,           // Max time to complete stroke
  dualHitWindowMs: 120,           // Window for dual hand detection
  deadTimeMs: 150,                // Debounce time between hits
  medianFilterSize: 5             // Velocity smoothing filter size
}
```

### Audio Settings
Located in `src/lib/audio.ts`:

```typescript
{
  masterVolume: 0.6,              // Overall volume (0-1)
  velocitySensitivity: 1.0,       // Velocity to volume mapping
  pitchVariation: 0.02,           // Random pitch variation (¬±2%)
  useCompressor: true             // Enable dynamics compression
}
```

### Model Selection
- **MoveNet Lightning**: Faster, lower latency (recommended)
- **MoveNet Thunder**: More accurate, slightly slower

## üß™ Testing

```bash
# Run unit tests
npm test

# Run tests with coverage
npm test -- --coverage

# Run tests in watch mode
npm test -- --watch
```

## üèóÔ∏è Build & Deploy

### Build for Production

```bash
npm run build
```

Output will be in `dist/` directory.

### Preview Production Build

```bash
npm run preview
```

### Deploy to GitHub Pages

```bash
npm run deploy
```

Deployment URL: `https://yunmaoleo.github.io/multi-model-input-web-app/`

## üåê Browser Compatibility

| Browser | Version | WebGPU | WebGL | WebAudio |
|---------|---------|--------|-------|----------|
| Chrome  | 90+     | ‚úÖ     | ‚úÖ    | ‚úÖ       |
| Edge    | 90+     | ‚úÖ     | ‚úÖ    | ‚úÖ       |
| Firefox | 88+     | ‚ö†Ô∏è     | ‚úÖ    | ‚úÖ       |
| Safari  | 14.5+   | ‚ùå     | ‚úÖ    | ‚úÖ       |

**Recommended**: Chrome 90+ for best performance (WebGPU support)

## üêõ Troubleshooting

### Audio Not Playing
**Symptoms**: Gesture detected but no sound
- ‚úÖ Check browser console for audio loading errors
- ‚úÖ Ensure audio files exist in `public/assets/drums/`
- ‚úÖ Verify audio context is initialized (look for "‚úÖ Audio system activated!" log)
- ‚úÖ Check browser audio permissions
- ‚úÖ Try refreshing the page and clicking "Start" again

### Gestures Not Detected
**Symptoms**: Hand movement doesn't trigger sounds
- ‚úÖ Check lighting - ensure your upper body is well-lit
- ‚úÖ Ensure wrist keypoints have >20% confidence (check console logs)
- ‚úÖ Make faster, more pronounced downward strokes
- ‚úÖ Complete strokes within 500ms
- ‚úÖ Move hand at least 8-10% of screen height
- ‚ö†Ô∏è If only lower body visible, wrist detection may fail

### Low FPS / Lag
- ‚úÖ Switch to MoveNet Lightning model
- ‚úÖ Close other browser tabs
- ‚úÖ Check GPU acceleration is enabled in browser settings
- ‚úÖ Lower camera resolution (edit `CameraFeed.tsx`)

### Camera Feed Issues
- ‚úÖ Grant camera permissions when prompted
- ‚úÖ Check if camera is used by another application
- ‚úÖ Try refreshing the page
- ‚úÖ Adjust blur effect in `CameraFeed.tsx` (`filter: blur(8px)`)

## üìö Key Implementation Details

### Gesture Detection Algorithm
1. **Keypoint Tracking**: Extract left/right wrist positions from MoveNet
2. **EMA Smoothing**: Apply alpha=0.7 smoothing for noise reduction
3. **Velocity Calculation**: Frame-rate independent (displacement / deltaTime)
4. **Stroke Detection**: 
   - Start: Detect downward motion (velocity > threshold)
   - Track: Accumulate downward displacement
   - Trigger: When accumulated distance > threshold AND within timeout
   - Reset: On upward motion or timeout

### Audio Pipeline
1. **Initialization**: Create AudioContext on user interaction
2. **Sample Loading**: Preload all WAV files using `fetch()` + `decodeAudioData()`
3. **Playback**: 
   - Map hand velocity to volume (logarithmic curve)
   - Apply random pitch variation (¬±2%)
   - Route through compressor for consistent loudness
4. **Performance**: Buffer pooling, no GC pressure during playback

### Visual Feedback
1. **Drum Pads**: Two elliptical pads rendered on canvas
2. **Hit Flash**: Intensity decays from 1.0 to 0 over ~17 frames (at 60fps)
3. **Color Mapping**: Green gradient ‚Üí bright green on hit
4. **Mirroring**: Labels and positions correctly mirrored for camera view

## üìñ Additional Documentation

- **[Research_Note.md](./Research_Note.md)** - Technical research and requirements
- **[BACKEND_PROMPT.md](./BACKEND_PROMPT.md)** - Backend integration guide (optional)
- **[CHANGELOG.md](./CHANGELOG.md)** - Version history and updates

## ü§ù Contributing

Contributions are welcome! Please follow these guidelines:
1. Fork the repository
2. Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## üìÑ License

MIT License - see LICENSE file for details

## üë§ Author

**YunmaoLeo**
- GitHub: [@YunmaoLeo](https://github.com/YunmaoLeo)

## üôè Acknowledgments

- TensorFlow.js team for MoveNet model
- WebAudio API specification contributors
- React and Vite communities

---

**Made with ‚ù§Ô∏è and TypeScript**
